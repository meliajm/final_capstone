{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal for Final Capstone: Forest Cover Type\n",
    "\n",
    "Melia Miller\n",
    "\n",
    "March 27, 2019\n",
    "\n",
    "\n",
    "## Problem to solve with a valuable solution\n",
    "\n",
    "The goal of this Capstone is to predict the forest cover type of an area in the Roosevelt National Forest of northern Colorado. While the study is limited to one specific national forest, the results and insights may be used for other National Forests, National Parks, and wilderness areas in and outside of the United States. This information can be used to track the health of forests, which forests are stable or growing/shrinking, and how forest types change over time. This may have far-reaching implications related to climate change, human expansion, and carbon dioxide levels. In addition, feature importance can be used to determine what factors influence the forest type in a given area.   \n",
    "\n",
    "Decision trees to be used to make decisions about actual trees! Initially, this dataset was collected to help accurately estimate natural resource inventory. This not only has the potential to have a global impact on estimating the type and area of forests, but also business implications for logging. If tools are available to predict the composition of wilderness areas and specifically the tree type; this information can be used to make decisions on which trees to cut, log, and sell, and which trees to plant again in the future.\n",
    "\n",
    "The goal for the final product is a simple interactive user-interface to understand the models and different features. In the future, the model could be expanded to include more wilderness areas to better understand forest ecosystems and ecological processes.\n",
    "\n",
    "Blackard, Jock A. and Denis J. Dean. 2000. \"Comparative Accuracies of Artificial Neural Networks and Discriminant Analysis in Predicting Forest Cover Types from Cartographic Variables.\" Computers and Electronics in Agriculture 24(3):131-151.\n",
    "\n",
    "https://pdfs.semanticscholar.org/42fd/f2999c46babe535974e14375fbb224445757.pdf\n",
    "\n",
    "## Data Source\n",
    "\n",
    "This data has been aggregated from US Geological Survey and US Forest Service (USFS) Region 2 Resource Information System data and includes forest types (Cover_Type) such as Spruce/Fir, Lodgepole Pine, Ponderosa Pine, Cottonwood/Willow, Aspen, Douglas-fir, and Krummholz. There are 12 cartographic variables. The cartographic variables are Elevation, Aspect in degrees azimuth, Slope in degrees, Horizontal_Distance_To_Hydrology (horizontal distance to nearest surface water features), Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am (0 to 255 index at summer solstice), Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points (distance to nearest wildfire ignition points), Wilderness_Area, and Soil_Type. There are four different Wilderness areas and 40 soil types. \n",
    "\n",
    "The dataset was put together by UCI and was used as a Kaggle competition 2015; the data files can be downloaded from Kaggle. The training set as over 15,000 rows and the test set has 566 rows.\n",
    "\n",
    "https://www.kaggle.com/c/forest-cover-type-kernels-only\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/covertype\n",
    "\n",
    "\n",
    "## Techniques to use for modeling\n",
    "\n",
    "I anticipate I will be using many supervised learning techniques such as k-Nearest Neighbors, Support Vector Machines, Decision Tree and Random Forest, logistic regression and LASSO regression, ensemble modeling such as gradient boosting, and neural networks for this classification problem.\n",
    "\n",
    "I will use some unsupervised learning techniques such as principal component analysis (PCA) for feature reduction and clustering to group data and get any other features.\n",
    "\n",
    "\n",
    "## Biggest challenge\n",
    "\n",
    "The plan for my data pipeline is to start with an exploration of the variables (determine value counts, check for missingness, do any data cleaning, describe, make histograms, check for outliers, determine correlation matrix, check for balances in outcome variable), I will then make many classification models including (kNN, SVM, logistic regression, random forest, and neural networks using Keras). I will evaluate each model using cross-validation, a classification report, and a confusion matrix. Final steps will be to determine important features, compare models based on their predictions and computational complexity, and create visuals to explain predictions. I anticipate the biggest challenge for me will be implementing my neural network and improving model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
